# 1. Import the InferencePipeline library
import os

# Configure to use Docker inference server (default port is 9001)
# Change this URL if your Docker container is running on a different port
os.environ["LOCAL_INFERENCE_API_URL"] = "http://localhost:9001"

from inference import InferencePipeline
import cv2

def my_sink(result, video_frame):
    if result.get("output_image"): # Display an image from the workflow response
        cv2.imshow("Workflow Image", result["output_image"].numpy_image)
        cv2.waitKey(1)
    # Do something with the predictions of each frame
    print(result)


# 2. Initialize a pipeline object
pipeline = InferencePipeline.init_with_workflow(
    api_key="OH6UuyaQOXGMccE2j6AT",
    workspace_name="edss",
    workflow_id="detect-count-and-visualize-2",
    video_reference=0, # Path to video, device id (int, usually 0 for built in webcams), or RTSP stream url
    max_fps=30,
    on_prediction=my_sink
)

# 3. Start the pipeline and wait for it to finish
pipeline.start()
pipeline.join()
